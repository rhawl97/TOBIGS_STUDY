{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the woman is a wise queen',\n",
    "          'the man is a wise president',\n",
    "          'she is a pretty woman',\n",
    "          'he is a strong man',\n",
    "          'she is still young',\n",
    "          'he is very old',\n",
    "          'he is the current president of US',\n",
    "          'the prince is a son of the king',\n",
    "          'the princess is a daughter of the king',\n",
    "          'a prince is a young man',\n",
    "          'a princess is a young woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1) 주석 부분에 function1의 목적을 쓰고, 그 주된 목적에 맞게 function1의 이름 변경하기\n",
    "def find_neigh(corpus):    #center word에 해당하는 neighbor word 찾기 \n",
    "    sentences = []\n",
    "    for sentence in corpus:  #단어별로 분리 \n",
    "        sentences.append(sentence.split())\n",
    "\n",
    "    window_size = 2 # q2) s의 의미를 적고 그에 맞게 변수명 변경하기 #s는 window size야!\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        for idx, word in enumerate(sentence):\n",
    "            for neighbor in sentence[max(idx - window_size, 0) : min(idx + window_size, len(sentence)) + 1] : \n",
    "                if neighbor != word: #neighbor이라는 변수에는 center word를 제외한 neighbor word만 담기 \n",
    "                    data.append([word, neighbor]) #center word와 neighbor word 하나씩 담기 ex)['the', 'woman'] ['the', 'is']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = function1(corpus)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woman</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is</td>\n",
       "      <td>wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input  label\n",
       "0    the  woman\n",
       "1    the     is\n",
       "2  woman    the\n",
       "3  woman     is\n",
       "4  woman      a\n",
       "5     is    the\n",
       "6     is  woman\n",
       "7     is      a\n",
       "8     is   wise\n",
       "9      a  woman"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'woman', 'queen', 'daughter', 'young', 'princess', 'a', 'old', 'prince', 'is', 'very', 'pretty', 'US', 'son', 'she', 'of', 'wise', 'president', 'current', 'strong', 'he', 'man', 'the', 'king', 'still'}\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "words = set(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3) 주석 부분에 function2의 목적을 쓰고, 그 목적에 맞게 function2의 이름 변경하기\n",
    "def tokenizing(corpus):\n",
    "    words = []\n",
    "    for text in corpus:\n",
    "        for word in text.split(' '): #tokenizing\n",
    "            words.append(word)\n",
    "    words = set(words)  #unique words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizing(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'woman': 0, 'queen': 1, 'daughter': 2, 'young': 3, 'princess': 4, 'a': 5, 'old': 6, 'prince': 7, 'is': 8, 'very': 9, 'pretty': 10, 'US': 11, 'son': 12, 'she': 13, 'of': 14, 'wise': 15, 'president': 16, 'current': 17, 'strong': 18, 'he': 19, 'man': 20, 'the': 21, 'king': 22, 'still': 23}\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for i,word in enumerate(words):\n",
    "    d[word] = i\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4) 주석 부분에 function3의 목적을 쓰고, 그 목적에 맞게 function3과 d의 이름을 변경하기\n",
    "def indexing(words): #한 token씩 차례로 one-hot encoding하기 위해 index를 생성 \n",
    "    d = {}\n",
    "    for i,word in enumerate(words):\n",
    "        d[word] = i\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = indexing(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# q5) 주석 부분에 function4의 목적을 쓰고, 그 목적에 맞게 function4와 ohe의 이름 변경하기\n",
    "def one_hot_encoding(word_index, ONE_HOT_DIM):  #for one-hot encoding #처음 단어를 컴퓨터가 인식할 수 있는 '숫자'로 바꾸기 위해!\n",
    "    ohe = np.zeros(ONE_HOT_DIM)\n",
    "    ohe[word_index] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q6) Word2Vec을 tensorflow로 구현한 코드에서 ? 부분을 올바르게 채워넣기\n",
    "# q7) 여기서 구현한 Word2Vec의 아키텍쳐는 CBOW or Skip Gram ?  #Skip Gram!\n",
    "# cross entropy 참고\n",
    "# https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/\n",
    "# https://kevinthegrey.tistory.com/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 24)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(function4(d[x], ONE_HOT_DIM))\n",
    "    Y.append(function4(d[y], ONE_HOT_DIM))\n",
    "\n",
    "# convert X,Y to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-283be03935a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mONE_HOT_DIM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# input word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(function4(d[x], ONE_HOT_DIM))\n",
    "    Y.append(function4(d[y], ONE_HOT_DIM))\n",
    "\n",
    "# convert X,Y to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "# placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, 24)) #24\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, 24)) #24\n",
    "\n",
    "# embedding dimension\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# hidden layer : represent word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM,2]))   #ONE_HOT_DIM,2(EMBEDDING_DIM)\n",
    "b1 = tf.Variable(tf.random_normal([1]))\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([ONE_HOT_DIM,2]))   #ONE_HOT_DIM,2(EMBEDDING_DIM)\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "output = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function : cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(output), axis=[1]))\n",
    "\n",
    "# training\n",
    "train = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 10000\n",
    "for i in range(iteration):\n",
    "    # input : X_train which is one hot encoded word\n",
    "    # label : Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 2000 == 0:\n",
    "        print('iteration '+ str(i) +' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hidden layer (W1 + b1) -> look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2 ))\n",
    "    \n",
    "PADDING = 1.0\n",
    "x1_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "x2_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x1_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "x2_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x1_axis_min,x1_axis_max)\n",
    "plt.ylim(x2_axis_min,x2_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

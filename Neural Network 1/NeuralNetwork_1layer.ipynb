{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    H = np.dot(X, W)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(H):\n",
    "    p = 1 / (1 + np.exp(-H))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.99966464987\n"
     ]
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H)\n",
    "p = sigmoid(H)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([-4, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10\n",
      "4.53978687024e-05\n"
     ]
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H)\n",
    "p = sigmoid(H)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"data\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = 0.0001 * np.random.randn(4, 3)  #4x3 행렬로 가중치 w 랜덤 생성(초기 가중치 설정)\n",
    "        self.params['b'] = np.ones(3)  #1x3 배열에 모두 1 넣기 #역시 linear function의 상수값에 대해 초기값 지정 \n",
    "    \n",
    "    def forward(self, X):\n",
    "        W = self.params['W']  #초기값 지정 \n",
    "        b = self.params['b']\n",
    "\n",
    "        h = np.dot(X, W) + b  #linear function \n",
    "        a = np.exp(h)  #softmax로 확률 구하기 위해 exponential 취하기 \n",
    "        #stable_a = np.exp(h - np.max(h, axis = 1).reshape(-1,1))  #loss nan값 방지 \n",
    "        p = a/np.sum(a, axis = 1).reshape(-1,1)  #softmax로 해당 label이 될 확률 p구하기   #1열에 맞게 행은 가변적으로 지정\n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, T):  #좋은 가중치 w를 구하기 위해 loss구하는 function 구현!\n",
    "        \n",
    "        p = self.forward(X)  #기존 가중치를 이용해 확률p 구하기 \n",
    "        \n",
    "        n = T.shape[0]  #실제 label의 행 수 저장 \n",
    "        log_likelihood = 0\n",
    "        log_likelihood -= np.log(p[np.arange(n), T]).sum()  #현재 구한 p를 softmax - Cross_entropy에 대해 미분 #equals P-T\n",
    "        Loss = log_likelihood / n\n",
    "\n",
    "        return Loss\n",
    "    \n",
    "    def accuracy(self, X, T):\n",
    "        p = self.forward(X) #예측\n",
    "        predict = np.argmax(p, axis = 1) #예측 결과 #확률이 가장 높은 인덱스가 예측된 label!\n",
    "        \n",
    "        return 1 - np.count_nonzero(predict - T)/len(T)\n",
    "        \n",
    "    def gradient(self, X, T, learning_rate = 0.0001):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        \n",
    "        t = np.zeros((T.shape[0], 3))\n",
    "        t[np.arange(T.shape[0]), T] = 1\n",
    "        #t는 인덱스 레이블 T를 One hot 벡터로 바꾼 것\n",
    "        \n",
    "        dp = p.copy()\n",
    "        dp[np.arange(len(T)), T] -= 1 #P 미분\n",
    "        \n",
    "        #목적함수에 대한 가중치 미분값을 담을 zero array 생성\n",
    "        grads = {}\n",
    "        grads['W'] = np.zeros((4, 3))\n",
    "        grads['b'] = np.zeros(10)\n",
    "        #목적함수에 대한 가중치 미분값 합 구하기\n",
    "        grads['W'] = (1/len(T)) * np.dot(X.T, p-t)  #chain rule 적용해 Loss의 w(가중치)에 대한 미분값 \n",
    "        grads['b'] = (1/len(T)) * np.sum(p-t, axis = 0)\n",
    "        #p-t 대신 dp 사용 가능\n",
    "        \n",
    "        self.params['W'] -= learning_rate * grads['W']  #loss를 minimize하는 방향으로 업데이트 \n",
    "        self.params['b'] -= learning_rate * grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 학습중입니다.\n",
      "Accuracy :  0.6466666666666667\n",
      "Loss :      1.09855297238\n",
      "[[  1.96573693e-04   4.93914075e-05   6.76184218e-05]\n",
      " [  1.30445304e-04   2.14928839e-04   2.37028269e-05]\n",
      " [ -7.80052698e-05   1.18282549e-04   3.06657993e-05]\n",
      " [  8.95771398e-05  -2.16292619e-06   1.15314668e-04]]\n",
      "1000 번째 학습중입니다.\n",
      "Accuracy :  0.33999999999999997\n",
      "Loss :      1.02009961798\n",
      "[[-0.00302049  0.0006832   0.00265087]\n",
      " [ 0.02415693 -0.01042444 -0.01336342]\n",
      " [-0.05790276  0.01478975  0.04318395]\n",
      " [-0.02533815  0.00350206  0.02203882]]\n",
      "2000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.964618405164\n",
      "[[ 0.00879421 -0.00032785 -0.00815277]\n",
      " [ 0.05422972 -0.02155237 -0.03230827]\n",
      " [-0.10198737  0.02761763  0.07444068]\n",
      " [-0.04584327  0.00632463  0.03972137]]\n",
      "3000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.916372262449\n",
      "[[ 0.0217381  -0.00156037 -0.01986415]\n",
      " [ 0.0833534  -0.03239023 -0.05059409]\n",
      " [-0.14192619  0.03945218  0.10254495]\n",
      " [-0.06458018  0.00873061  0.0560523 ]]\n",
      "4000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.874195935922\n",
      "[[ 0.03388235 -0.00251459 -0.03105417]\n",
      " [ 0.11062352 -0.04265767 -0.06759677]\n",
      " [-0.17920372  0.05058029  0.12869438]\n",
      " [-0.08205678  0.01080868  0.07145082]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    softmax.gradient(x, y)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(i, \"번째 학습중입니다.\")\n",
    "        print(\"Accuracy : \", softmax.accuracy(x, y))\n",
    "        print(\"Loss :     \", softmax.loss(x, y))\n",
    "        print(softmax.params['W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# CenterCrop : 이미지 가운데 부분만 잘라서 사용하겠다\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "# 이미지 전처리를 위한 transforms.Compose\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, \n",
    "                         batch_size=4, #batchsize 4개 지정해서 12500개의 데이터를 불러오겠다 \n",
    "                         shuffle=True, \n",
    "                         drop_last=False)\n",
    "\n",
    "testloader = DataLoader(testset, \n",
    "                        batch_size=4,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([5, 8, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "trainiter = iter(trainloader) #한 batch만 뽑아서 확인해보기 \n",
    "images, labels = trainiter.next() #batch size 4 로 지정한 shape확인 가능 \n",
    "\n",
    "print(len(trainloader))\n",
    "print(images.shape)\n",
    "print(labels) #1차원 tensor인데 length 가 4인! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Model - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = {'input_size':3*32*32,   #config #환경설정 #들어가야하는 input들 한번에 묶어놓은 것 \n",
    "       'hidden_size1':256,\n",
    "       'hidden_size2':128,\n",
    "       'hidden_size3':64,\n",
    "       'output_size':10,\n",
    "       'init_weight_range':0.5,\n",
    "       'num_epochs':5,\n",
    "       'batch_size':4,\n",
    "       'learning_rate':1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CifarModel_Yum(nn.Module):\n",
    "    def __init__(self, src):\n",
    "        super(CifarModel_Yum, self).__init__()\n",
    "        self.fc1 = nn.Linear(src['input_size'], src['hidden_size1']) #인자:input값의 개수 , output값의 개수  #층을 하나 만드는 것!\n",
    "        self.fc2 = nn.Linear(src['hidden_size1'], src['hidden_size2'])\n",
    "        self.fc3 = nn.Linear(src['hidden_size2'], src['hidden_size3'])\n",
    "        self.fc4 = nn.Linear(src['hidden_size3'], src['output_size']) #최종 length가 10인 tensor가 나옴\n",
    "                             \n",
    "        ## sequential layer\n",
    "        self.seq_fc = nn.Sequential(\n",
    "                            nn.Linear(src['input_size'], src['hidden_size1']),\n",
    "                            nn.Linear(src['hidden_size1'], src['hidden_size2']),\n",
    "                            nn.Linear(src['hidden_size2'], src['hidden_size3']),\n",
    "                            nn.Linear(src['hidden_size3'], src['output_size'])\n",
    "                            )   #하나의 layer로 묶음  #위랑 결과 같음!\n",
    "        \n",
    "        self.init_range = src['init_weight_range']\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.fc1.weight.data.uniform_(-self.init_range, self.init_range) #가중치 초기화 \n",
    "        self.fc2.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        self.fc3.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        self.fc4.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        \n",
    "        for fc in self.seq_fc:\n",
    "            fc.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        \n",
    "    def forward(self, img): #4 layers \n",
    "        x = img.view(img.shape[0], -1) #image shape 4*3*32*32 중 4차원만 유지하고 나머지는 한 차원으로 묶어버리겠다\n",
    "        #--------------------        \n",
    "        x = F.relu(self.fc1(x)) #activation function으로 ReLU 사용 \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        y = self.fc4(x)\n",
    "        #--------------------\n",
    "        \n",
    "        ## sequencial로 써도 위와 같은 모델 \n",
    "        #  y = self.seq_fc(x)\n",
    "\n",
    "        return y #마지막 y는 length10짜리 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CifarModel_Yum(src) #init하는 부분 \n",
    "y = model(images) #forward부분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train / Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수 및 optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  #loss\n",
    "optimizer = optim.SGD(model.parameters(), #gradient SGD\n",
    "                      src['learning_rate'], \n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 학습\n",
    "단순히 데이터를 반복시켜 네트워크와 optimizer의 입력으로 넘겨주기만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1000/12000], Loss: 4.4639\n",
      "Epoch [1/5], Step [2000/12000], Loss: 2.3055\n",
      "Epoch [1/5], Step [3000/12000], Loss: 2.3030\n",
      "Epoch [1/5], Step [4000/12000], Loss: 2.3082\n",
      "Epoch [1/5], Step [5000/12000], Loss: 2.3050\n",
      "Epoch [1/5], Step [6000/12000], Loss: 2.3029\n",
      "Epoch [1/5], Step [7000/12000], Loss: 2.3034\n",
      "Epoch [1/5], Step [8000/12000], Loss: 2.3031\n",
      "Epoch [1/5], Step [9000/12000], Loss: 2.3031\n",
      "Epoch [1/5], Step [10000/12000], Loss: 2.3032\n",
      "Epoch [1/5], Step [11000/12000], Loss: 2.3028\n",
      "Epoch [1/5], Step [12000/12000], Loss: 2.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████                                | 1/5 [16:42<1:06:50, 1002.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1000/12000], Loss: 2.3033\n",
      "Epoch [2/5], Step [2000/12000], Loss: 2.3045\n",
      "Epoch [2/5], Step [3000/12000], Loss: 2.3028\n",
      "Epoch [2/5], Step [4000/12000], Loss: 2.3036\n",
      "Epoch [2/5], Step [5000/12000], Loss: 2.3031\n",
      "Epoch [2/5], Step [6000/12000], Loss: 2.3035\n",
      "Epoch [2/5], Step [7000/12000], Loss: 2.3024\n",
      "Epoch [2/5], Step [8000/12000], Loss: 2.3037\n",
      "Epoch [2/5], Step [9000/12000], Loss: 2.3031\n",
      "Epoch [2/5], Step [10000/12000], Loss: 2.3034\n",
      "Epoch [2/5], Step [11000/12000], Loss: 2.3032\n",
      "Epoch [2/5], Step [12000/12000], Loss: 2.3037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████████████████▊                         | 2/5 [38:56<55:05, 1101.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1000/12000], Loss: 2.3020\n",
      "Epoch [3/5], Step [2000/12000], Loss: 2.3031\n",
      "Epoch [3/5], Step [3000/12000], Loss: 2.3031\n",
      "Epoch [3/5], Step [4000/12000], Loss: 2.3033\n",
      "Epoch [3/5], Step [5000/12000], Loss: 2.3031\n",
      "Epoch [3/5], Step [6000/12000], Loss: 2.3034\n",
      "Epoch [3/5], Step [7000/12000], Loss: 2.3028\n",
      "Epoch [3/5], Step [8000/12000], Loss: 2.3024\n",
      "Epoch [3/5], Step [9000/12000], Loss: 2.3032\n",
      "Epoch [3/5], Step [10000/12000], Loss: 2.3033\n",
      "Epoch [3/5], Step [11000/12000], Loss: 2.3034\n",
      "Epoch [3/5], Step [12000/12000], Loss: 2.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|████████████████████████                | 3/5 [1:03:38<40:31, 1216.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1000/12000], Loss: 2.3034\n",
      "Epoch [4/5], Step [2000/12000], Loss: 2.3034\n",
      "Epoch [4/5], Step [3000/12000], Loss: 2.3025\n",
      "Epoch [4/5], Step [4000/12000], Loss: 2.3038\n",
      "Epoch [4/5], Step [5000/12000], Loss: 2.3033\n",
      "Epoch [4/5], Step [6000/12000], Loss: 2.3034\n",
      "Epoch [4/5], Step [7000/12000], Loss: 2.3027\n",
      "Epoch [4/5], Step [8000/12000], Loss: 2.3038\n",
      "Epoch [4/5], Step [9000/12000], Loss: 2.3028\n",
      "Epoch [4/5], Step [10000/12000], Loss: 2.3028\n",
      "Epoch [4/5], Step [11000/12000], Loss: 2.3030\n",
      "Epoch [4/5], Step [12000/12000], Loss: 2.3030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████        | 4/5 [1:28:35<21:40, 1300.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1000/12000], Loss: 2.3035\n",
      "Epoch [5/5], Step [2000/12000], Loss: 2.3029\n",
      "Epoch [5/5], Step [3000/12000], Loss: 2.3024\n",
      "Epoch [5/5], Step [4000/12000], Loss: 2.3036\n",
      "Epoch [5/5], Step [5000/12000], Loss: 2.3031\n",
      "Epoch [5/5], Step [6000/12000], Loss: 2.3031\n",
      "Epoch [5/5], Step [7000/12000], Loss: 2.3035\n",
      "Epoch [5/5], Step [8000/12000], Loss: 2.3034\n",
      "Epoch [5/5], Step [9000/12000], Loss: 2.3029\n",
      "Epoch [5/5], Step [10000/12000], Loss: 2.3034\n",
      "Epoch [5/5], Step [11000/12000], Loss: 2.3023\n",
      "Epoch [5/5], Step [12000/12000], Loss: 2.3032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████| 5/5 [1:53:20<00:00, 1355.63s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  \n",
    "\n",
    "model.init_weight()\n",
    "\n",
    "for epoch in tqdm(range(src['num_epochs'])):   #epoch 5번 \n",
    "    current_loss = 0.0\n",
    "#     model.train(True)\n",
    "    \n",
    "    for i, data in enumerate(trainloader): #4개씩 묶여있는 사진들이 루프 한번만에!\n",
    "        # get the inputs\n",
    "        inputs, labels = data  #image에 사진이랑 클래스 있음 \n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()  #0으로 초기화 \n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) #실제 사진과 확률의 차이\n",
    "        loss.backward()  #편미분 값들 계산\n",
    "        optimizer.step()  #계산한 값들로 update\n",
    "        \n",
    "        # print statistics\n",
    "        step = i + 1\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        if step % 1000 == 0 and step != 0:     # print every 1000 mini-batches\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  (epoch + 1, src['num_epochs'], step, len(trainloader)//1000 * 1000, current_loss / 1000)) #누적합의 평균 \n",
    "            current_loss = 0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 데이터를 이용한 네트워크 평가\n",
    "학습 데이터셋을 이용해 총 5번 반복하면서 학습을 시켰다. 그러나 실제로 네트워크가 무엇인가를 배웠는지에 대하여 테스트를 해야한다.\n",
    "\n",
    "뉴럴 네트워크의 출력인 클래스 label을 예측하고 실제 데이터와 비교함으로써 테스트를 수행할 수 있는데 만약 예측 결과가 올바르다면 올바른 예측 리스트에 샘플을 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 2500 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, labels = data\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 너무 낮아서 다른 시도를 해보기로 한당,,, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2. Model - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarModel_sigmoid(nn.Module):\n",
    "    def __init__(self, src):\n",
    "        super(CifarModel_sigmoid, self).__init__()\n",
    "        self.fc1 = nn.Linear(src['input_size'], src['hidden_size1']) #인자:input값의 개수 , output값의 개수  #층을 하나 만드는 것!\n",
    "        self.fc2 = nn.Linear(src['hidden_size1'], src['hidden_size2'])\n",
    "        self.fc3 = nn.Linear(src['hidden_size2'], src['hidden_size3'])\n",
    "        self.fc4 = nn.Linear(src['hidden_size3'], src['output_size']) #최종 length가 10인 tensor가 나옴\n",
    "                             \n",
    "        ## sequential layer\n",
    "        self.seq_fc = nn.Sequential(\n",
    "                            nn.Linear(src['input_size'], src['hidden_size1']),\n",
    "                            nn.Linear(src['hidden_size1'], src['hidden_size2']),\n",
    "                            nn.Linear(src['hidden_size2'], src['hidden_size3']),\n",
    "                            nn.Linear(src['hidden_size3'], src['output_size'])\n",
    "                            )   #하나의 layer로 묶음  #위랑 결과 같음!\n",
    "        \n",
    "        self.init_range = src['init_weight_range']\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.fc1.weight.data.uniform_(-self.init_range, self.init_range) #가중치 초기화 \n",
    "        self.fc2.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        self.fc3.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        self.fc4.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        \n",
    "        for fc in self.seq_fc:\n",
    "            fc.weight.data.uniform_(-self.init_range, self.init_range)\n",
    "        \n",
    "    def forward(self, img): #4 layers \n",
    "        x = img.view(img.shape[0], -1) #image shape 4*3*32*32 중 4차원만 유지하고 나머지는 한 차원으로 묶어버리겠다\n",
    "        #--------------------  \n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x)) #activation function으로 sigmoid 사용 \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        y = self.fc4(x)\n",
    "        #--------------------\n",
    "        \n",
    "        ## sequencial로 써도 위와 같은 모델 \n",
    "        #  y = self.seq_fc(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CifarModel_sigmoid(src) #init하는 부분 #준비물들 준비 \n",
    "y = model(images) #forward부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1000/12000], Loss: 3.1298\n",
      "Epoch [1/5], Step [2000/12000], Loss: 3.1061\n",
      "Epoch [1/5], Step [3000/12000], Loss: 3.1346\n",
      "Epoch [1/5], Step [4000/12000], Loss: 3.1063\n",
      "Epoch [1/5], Step [5000/12000], Loss: 3.1490\n",
      "Epoch [1/5], Step [6000/12000], Loss: 3.1297\n",
      "Epoch [1/5], Step [7000/12000], Loss: 3.1167\n",
      "Epoch [1/5], Step [8000/12000], Loss: 3.1246\n",
      "Epoch [1/5], Step [9000/12000], Loss: 3.1337\n",
      "Epoch [1/5], Step [10000/12000], Loss: 3.1651\n",
      "Epoch [1/5], Step [11000/12000], Loss: 3.1408\n",
      "Epoch [1/5], Step [12000/12000], Loss: 3.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████                                | 1/5 [25:35<1:42:20, 1535.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1000/12000], Loss: 3.0936\n",
      "Epoch [2/5], Step [2000/12000], Loss: 3.1109\n",
      "Epoch [2/5], Step [3000/12000], Loss: 3.1167\n",
      "Epoch [2/5], Step [4000/12000], Loss: 3.1375\n",
      "Epoch [2/5], Step [5000/12000], Loss: 3.1531\n",
      "Epoch [2/5], Step [6000/12000], Loss: 3.1224\n",
      "Epoch [2/5], Step [7000/12000], Loss: 3.1282\n",
      "Epoch [2/5], Step [8000/12000], Loss: 3.0965\n",
      "Epoch [2/5], Step [9000/12000], Loss: 3.1335\n",
      "Epoch [2/5], Step [10000/12000], Loss: 3.1369\n",
      "Epoch [2/5], Step [11000/12000], Loss: 3.1636\n",
      "Epoch [2/5], Step [12000/12000], Loss: 3.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████████████████                        | 2/5 [52:00<1:17:30, 1550.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1000/12000], Loss: 3.1684\n",
      "Epoch [3/5], Step [2000/12000], Loss: 3.0771\n",
      "Epoch [3/5], Step [3000/12000], Loss: 3.1671\n",
      "Epoch [3/5], Step [4000/12000], Loss: 3.1191\n",
      "Epoch [3/5], Step [5000/12000], Loss: 3.1194\n",
      "Epoch [3/5], Step [6000/12000], Loss: 3.1222\n",
      "Epoch [3/5], Step [7000/12000], Loss: 3.1347\n",
      "Epoch [3/5], Step [8000/12000], Loss: 3.1392\n",
      "Epoch [3/5], Step [9000/12000], Loss: 3.1373\n",
      "Epoch [3/5], Step [10000/12000], Loss: 3.0963\n",
      "Epoch [3/5], Step [11000/12000], Loss: 3.0898\n",
      "Epoch [3/5], Step [12000/12000], Loss: 3.1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|████████████████████████                | 3/5 [1:17:54<51:43, 1551.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1000/12000], Loss: 3.1559\n",
      "Epoch [4/5], Step [2000/12000], Loss: 3.1205\n",
      "Epoch [4/5], Step [3000/12000], Loss: 3.1083\n",
      "Epoch [4/5], Step [4000/12000], Loss: 3.1581\n",
      "Epoch [4/5], Step [5000/12000], Loss: 3.1188\n",
      "Epoch [4/5], Step [6000/12000], Loss: 3.1118\n",
      "Epoch [4/5], Step [7000/12000], Loss: 3.1114\n",
      "Epoch [4/5], Step [8000/12000], Loss: 3.1395\n",
      "Epoch [4/5], Step [9000/12000], Loss: 3.1264\n",
      "Epoch [4/5], Step [10000/12000], Loss: 3.1334\n",
      "Epoch [4/5], Step [11000/12000], Loss: 3.0924\n",
      "Epoch [4/5], Step [12000/12000], Loss: 3.0989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████        | 4/5 [1:43:45<25:51, 1551.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1000/12000], Loss: 3.0721\n",
      "Epoch [5/5], Step [2000/12000], Loss: 3.1033\n",
      "Epoch [5/5], Step [3000/12000], Loss: 3.1274\n",
      "Epoch [5/5], Step [4000/12000], Loss: 3.1523\n",
      "Epoch [5/5], Step [5000/12000], Loss: 3.1514\n",
      "Epoch [5/5], Step [6000/12000], Loss: 3.1017\n",
      "Epoch [5/5], Step [7000/12000], Loss: 3.1119\n",
      "Epoch [5/5], Step [8000/12000], Loss: 3.1556\n",
      "Epoch [5/5], Step [9000/12000], Loss: 3.1230\n",
      "Epoch [5/5], Step [10000/12000], Loss: 3.1572\n",
      "Epoch [5/5], Step [11000/12000], Loss: 3.1192\n",
      "Epoch [5/5], Step [12000/12000], Loss: 3.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████| 5/5 [2:09:47<00:00, 1554.60s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "model.init_weight()\n",
    "\n",
    "for epoch in tqdm(range(src['num_epochs'])):    \n",
    "    current_loss = 0.0\n",
    "#     model.train(True)\n",
    "    \n",
    "    for i, data in enumerate(trainloader): \n",
    "        # get the inputs\n",
    "        inputs, labels = data   \n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "    \n",
    "        step = i + 1\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        if step % 1000 == 0 and step != 0:    \n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  (epoch + 1, src['num_epochs'], step, len(trainloader)//1000 * 1000, current_loss / 1000))  \n",
    "            current_loss = 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 데이터를 이용한 네트워크 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 2500 test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, labels = data\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
